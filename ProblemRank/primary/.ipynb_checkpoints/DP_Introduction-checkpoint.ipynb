{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、动态规划思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们来回顾一下斐波那契数列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fib.png\" width = \"500\" height = \"100\" div align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从斐波那契数列的递推公式我们可以看出存在递归关系，而且这种递归关系存在一种重叠子问题（overlap sub-problem）当我们使用递归方法来计算斐波那契数列时如下图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fib_recursion.png\" width = \"500\" height = \"100\" div align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们会发现fib(5),fib(4)...会重复计算，这样会大大增加算法的复杂度，如果我们用一个数组来记录重复的子问题需要的时候直接调用那么就可以大大减小复杂度。动态规划方法可以用来解决这些重叠子问题。我们可以通过<a href=\"https://blog.csdn.net/tyhj_sf/article/details/53969072\">该博客</a>来了解递归、分治、动态规划的关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有数组arr = \\[1, 2, 4, 1, 7, 8, 3 \\]从该数组中选取不相邻的数使得和最大，这是一道经典的动态规划——选与不选的题目，每个数组元素都有选与不选两个状态。当i=6时:如果选arr\\[ i \\]那么最佳结果为arr\\[ i \\]+opt(arr\\[ 0 - i-2 \\]);如果不选arr\\[ i \\]那么最佳结果为arr\\[ i-1 \\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "arr = [1, 2, 4, 1, 7, 8, 3]\n",
    "\n",
    "def rec_opt(arr, i): #递归解法\n",
    "    if i==0:#递归出口1\n",
    "        return arr[i]\n",
    "    elif i==1:#递归出口2\n",
    "        return max(arr[0], arr[1])\n",
    "    else:#递归\n",
    "        a = rec_opt(arr,i-2) + arr[i]#选i\n",
    "        b = rec_opt(arr,i-1)#不选i\n",
    "        return max(a,b)\n",
    "    \n",
    "\n",
    "#递归思想将大问题分解成小问题，小问题之间可能存在重叠；动态规划思想是先计算小问题累计到大问题。\n",
    "#他们的共同特点是需要找到递推关系式\n",
    "\n",
    "def dp_opt(arr):\n",
    "    opt = [0, 0, 0, 0, 0, 0, 0] #记录每个节点的最优解\n",
    "    opt[0] = arr[0]             #从子问题开始\n",
    "    opt[1] = max(arr[0],arr[1])\n",
    "    for i in range(2, len(arr)): #递推关系\n",
    "        a = arr[i] + opt[i-2]\n",
    "        b = opt[i-1]\n",
    "        opt[i] = max(a,b)\n",
    "    return opt[len(arr)-1]\n",
    "\n",
    "print(rec_opt(arr, 6))\n",
    "print(dp_opt(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、 DP思想求解HMM观测序列概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 引入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现实生活中我们常常遇到这种问题：存在一个可观察序列（o<sub>1</sub>  o<sub>2</sub>, o<sub>3</sub>, ... o<sub>t</sub>）和一个隐藏序列（h<sub>1</sub>  h<sub>2</sub>, h<sub>3</sub>, ... h<sub>t</sub>）（所谓隐藏序列就是我们无法得知的序列）问题是我们如何根据可观测序列来“猜”隐藏序列呢？这是一个很有实际意义的问题，比如：我们现在用的输入法，用户输入的是拼音字母，输入法的任务是“猜”你想输入的汉字；自动医疗诊断系统根据你说的话（%.##..*@）来\"猜\"你的病情（健康or感冒or发烧）。等等，隐马尔可夫模型（HHM）的应用场景实在太多了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 隐马尔可夫模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设X是所有可观测状态的集合，集合大小为M；Y是所有隐藏状态的集合，集合大小为N。对于一个长度为T的序列，O（o<sub>1</sub>  o<sub>2</sub>, o<sub>3</sub>, ... o<sub>T</sub>）为对应的观测序列；H（h<sub>1</sub>  h<sub>2</sub>, h<sub>3</sub>, ... h<sub>T</sub>）为对应的隐藏序列。<br>HMM 做了两个假设：<br>(1)马尔科夫假设:任意时刻的隐藏状态只依赖于它前一个隐藏状态。（这个假设不够准确但是简化了问题，这个在该文不做讨论）如果在t时刻的隐藏状态为s<sub>t</sub> = h<sub>i</sub>在t+1时刻的隐藏状态为s<sub>t+1</sub> =h<sub>j</sub>，从t时刻转到t+1时刻的状态转移概率为a<sub>ij</sub> = P(s<sub>t+1</sub> =h<sub>j</sub>|s<sub>t</sub> = h<sub>i</sub>)。这样就形成了一个大小为N x N的状态转移矩阵A = \\[a<sub>ij</sub>\\]<sub>N x N</sub>.<br>如下图：i、j、k三个隐藏状态及对应的状态转移概率矩阵：<br>\n",
    "<img src=\"./images/HMM_1.png\" width = \"400\" height = \"100\" div align=left /><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在t时刻的隐藏状态为s<sub>t</sub> = h<sub>i</sub>而对应的观察状态为p<sub>t</sub> = o<sub>j</sub>，则该时刻观察状态o<sub>j</sub>在隐藏状态 h<sub>i</sub>下生成的概率为b<sub>ij</sub> = P(p<sub>j</sub> = o<sub>j</sub>\n",
    "|s<sub>i</sub> = h<sub>i</sub>)。这样就形成了一个大小为N x M的状态生成矩阵A = \\[b<sub>ij</sub>\\]<sub>N x M</sub>.<br>另外一个HMM模型需要隐藏状态初始概率分布C=\\[c(𝑖)\\]<sub>N</sub>。其中c(𝑖)=P(s = h<sub>i</sub>)。因此，HMM模型可以由一个三元组𝜆表示如下：HHM =（A，B，C）<img src=\"./images/HMM_2.png\" width = \"500\" height = \"100\" div align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们用一个简单的实例来描述上面抽象出的HMM模型。这是一个盒子与球的模型，例子来源于李航的《统计学习方法》。假设我们有3个盒子，每个盒子里都有红色和白色两种球，这三个盒子里球的数量分别是：<br>\n",
    "<img src=\"./images/hezi.png\" width = \"500\" height = \"100\" div align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照下面的方法从盒子里抽球，开始的时候，从第一个盒子抽球的概率是0.2，从第二个盒子抽球的概率是0.4，从第三个盒子抽球的概率是0.4。以这个概率抽一次球后，将球放回。然后从当前盒子转移到下一个盒子进行抽球。规则是：如果当前抽球的盒子是第一个盒子，则以0.5的概率仍然留在第一个盒子继续抽球，以0.2的概率去第二个盒子抽球，以0.3的概率去第三个盒子抽球。如果当前抽球的盒子是第二个盒子，则以0.5的概率仍然留在第二个盒子继续抽球，以0.3的概率去第一个盒子抽球，以0.2的概率去第三个盒子抽球。如果当前抽球的盒子是第三个盒子，则以0.5的概率仍然留在第三个盒子继续抽球，以0.2的概率去第一个盒子抽球，以0.3的概率去第二个盒子抽球。如此下去，直到重复三次，得到一个球的颜色的观测序列:𝑂={红，白，红}<br> 注意在这个过程中，观察者只能看到球的颜色序列，却不能看到球是从哪个盒子里取出的。那么按照我们上一节HMM模型的定义，我们的观察集合是:𝑉={红，白}，𝑀=2。我们的状态集合是：𝑄={盒子1，盒子2，盒子3}，𝑁=3。而观察序列和状态序列的长度为3。初始状态分布为：C=(0.2,0.4,0.4)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>状态转移概率分布矩阵为：<br><img src=\"./images/zhenju2.png\" width = \"300\" height = \"100\" div align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>观测状态生成概率矩阵为：<br><img src=\"./images/juzhen1.png\" width = \"300\" height = \"100\" div align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.动态规划求解HMM观测序列概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 暴力求解法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们知道所有的隐藏状态之间的转移概率和所有从隐藏状态到观测状态生成概率，那么我们是可以暴力求解的。我们可以列举出所有可能出现的长度为𝑇的隐藏序列H=（h<sub>1</sub>  h<sub>2</sub>, h<sub>3</sub>, ... h<sub>T</sub>）（N<sup>T</sup>种不同的序列）分布求出这些隐藏序列与观测序列𝑂=（o<sub>1</sub>  o<sub>2</sub>, o<sub>3</sub>, ... o<sub>t</sub>）的联合概率分布𝑃(𝑂,H|HMM)。这样我们就可以很容易的求出边缘分布𝑃(𝑂|HMM)了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然上述方法有效，但是如果我们的隐藏状态数𝑁非常多的那就麻烦了，此时我们预测状态有𝑁<sup>𝑇</sup>种组合，算法的时间复杂度是𝑂(𝑇𝑁<sup>𝑇</sup>)阶的。因此对于一些隐藏状态数极少的模型，我们可以用暴力求解法来得到观测序列出现的概率，但是如果隐藏状态多，则上述算法太耗时，我们需要寻找其他简洁的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 用前向算法求HMM观测序列的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向算法本质上属于动态规划的算法，递归思想将大问题分解成小问题，小问题之间可能存在重叠；动态规划思想是先计算小问题累计到大问题。他们的共同特点是需要找到递推关系式。<br>接下来我们来寻找递推关系式将大问题分解为小问题：<br>假设我们只有一个观测状态这个观测状态的概率𝑃(o<sub>1</sub>|HMM)=$\\sum_{i=1}^N$(𝑃(h<sub>i</sub>|HMM)*b<sub>i1</sub>),即，产生观测状态o<sub>1</sub>的隐藏状态有很多将每种隐藏状态乘它对应的生成观测状态概率然后求和。则当序列长度为T时𝑃(o<sub>1</sub>  o<sub>2</sub>, o<sub>3</sub>, ... o<sub>t</sub>|HMM)=$\\sum_{i=1}^N$(𝑃(o<sub>1</sub>,o<sub>2</sub>,o<sub>3</sub>, ... o<sub>t-1</sub>,h<sub>i</sub>|HMM)*b <sub>t-1 t,</sub>),即将产生(o<sub>1</sub>,o<sub>2</sub>,o<sub>3</sub>, ... o<sub>t-1</sub>）状态序列的概率分别乘上h<sub>t</sub>的状态转换概率然后乘上对应的生成概率再求和。我们以上面介绍的抽球实验为例求解观测序列概率："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "球的颜色的观测序列:𝑂={红，白}<br>\n",
    "时刻1是红色球<br>隐藏状态是盒子1的概率为：𝛼<sub>1</sub>(1)=c<sub>1</sub>𝑏<sub>11</sub>=0.2×0.5=0.1;<br>隐藏状态是盒子2的概率为：𝛼<sub>1</sub>(2)=c<sub>2</sub>𝑏<sub>21</sub>=0.2×0.5=0.1;<br>隐藏状态是盒子3的概率为：𝛼<sub>1</sub>(3)=c<sub></sub>𝑏<sub>31</sub>=0.2×0.5=0.1<br>时刻2是白色球<br>隐藏状态是盒子1的概率为：𝛼<sub>2</sub>(1)=\\[$\\sum_{i=1}^3$𝛼<sub>1</sub>(i)a<sub>i1</sub>\\]*b<sub>12</sub>=\\[0.1∗0.5+0.16∗0.3+0.28∗0.2\\]×0.5=0.077;<br>隐藏状态是盒子2的概率为：𝛼<sub>2</sub>(2)=\\[$\\sum_{i=1}^3$𝛼<sub>1</sub>(i)a<sub>i2</sub>\\]*b<sub>22</sub>=\\[0.1∗0.2+0.16∗0.5+0.28∗0.3\\]×0.6=0.1104;<br>隐藏状态是盒子3的概率为：𝛼<sub>2</sub>(3)=\\[$\\sum_{i=1}^3$𝛼<sub>1</sub>(i)a<sub>i3</sub>\\]*b<sub>32</sub>=\\[0.1∗0.3+0.16∗0.2+0.28∗0.5\\]×0.3=0.0606;<br>最终我们求出观测序列:𝑂={红，白}的概率为：<br>𝑃(𝑂|𝜆)=$\\sum_{i=1}^3$𝛼2(𝑖) = 0.248\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
